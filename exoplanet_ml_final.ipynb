{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59de3a74",
   "metadata": {},
   "source": [
    "\n",
    "# Exoplanet ML — Ensamble + Umbral para Bajar Falsos Positivos\n",
    "\n",
    "**Fecha:** 2025-10-04 19:40:37\n",
    "\n",
    "Este notebook unifica la evaluación de **RandomForest**, **ExtraTrees**, **AdaBoost** y un **Stacking** (LogReg meta) con un **mismo protocolo CV estratificado**.  \n",
    "Incluye:\n",
    "- **OOF evaluation** con `cross_val_predict` para métricas estables (PR-AUC, ROC-AUC, precisión, recall, F1).\n",
    "- **Curva Precisión–Recall** y **selección de umbral** para controlar falsos positivos (priorizar precisión mínima).\n",
    "- **Calibración** de probabilidades (`CalibratedClassifierCV`) antes de fijar el umbral.\n",
    "- Exportación del **modelo final + umbral** y un `registry.json` con todas las métricas.\n",
    "\n",
    "> **TODO**: Completá las rutas/columnas de tus datos en la celda *Carga de datos*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c08f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== Setup & reproducibilidad ====\n",
    "import os, warnings, json, joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_predict\n",
    "from sklearn.metrics import (roc_auc_score, average_precision_score, precision_recall_curve,\n",
    "                             confusion_matrix, precision_score, recall_score, f1_score, roc_curve)\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.random.seed(42)\n",
    "\n",
    "# Directorios\n",
    "MODELS_DIR = Path(\"models\")\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "REGISTRY_PATH = MODELS_DIR / \"registry.json\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9a246c",
   "metadata": {},
   "source": [
    "## Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7566c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== Carga de datos ====\n",
    "# OPCIÓN A (CSV): descomentar y definir\n",
    "# DATA_PATH = \"data/dataset.csv\"\n",
    "# TARGET_COL = \"label\"\n",
    "# df = pd.read_csv(DATA_PATH)\n",
    "# y = df[TARGET_COL].astype(int).values\n",
    "# feature_cols = [c for c in df.columns if c != TARGET_COL]\n",
    "# X = df[feature_cols].values\n",
    "\n",
    "# OPCIÓN B (X, y ya construidos en memoria): si ejecutás el notebook dentro de tu proyecto, podés\n",
    "# simplemente asignar X, y antes y saltar esta celda.\n",
    "\n",
    "# OPCIÓN C (usar features.json para columnas) — si lo tenés disponible:\n",
    "feature_cols = None\n",
    "if Path(\"/mnt/data/features.json\").exists():\n",
    "    try:\n",
    "        with open(\"/mnt/data/features.json\", \"r\") as f:\n",
    "            feature_cols = json.load(f)\n",
    "            print(\"features.json detectado con\", len(feature_cols), \"features.\")\n",
    "    except Exception as e:\n",
    "        print(\"No se pudo leer features.json:\", e)\n",
    "\n",
    "# Si ya tenés X, y en el entorno, dejá esta celda como informativa:\n",
    "try:\n",
    "    X\n",
    "    y\n",
    "    print(\"X,y ya existen. Shape:\", np.shape(X), \"y positivos:\", int(np.sum(y)))\n",
    "    if feature_cols is not None and isinstance(X, np.ndarray) and X.shape[1] == len(feature_cols):\n",
    "        print(\"Las columnas de features.json coinciden con X.shape[1].\")\n",
    "except NameError:\n",
    "    print(\">> Definí X,y o cargalos desde CSV. Ver instrucciones arriba.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1d1a55",
   "metadata": {},
   "source": [
    "## Protocolo de evaluación y utilidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397bdfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== Protocolo de evaluación ====\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "def pr_metrics_from_probs(y_true, y_prob, threshold=0.5):\n",
    "    ap = average_precision_score(y_true, y_prob)  # PR-AUC\n",
    "    roc = roc_auc_score(y_true, y_prob)\n",
    "    y_hat = (y_prob >= threshold).astype(int)\n",
    "    prec = precision_score(y_true, y_hat, zero_division=0)\n",
    "    rec  = recall_score(y_true, y_hat, zero_division=0)\n",
    "    f1   = f1_score(y_true, y_hat, zero_division=0)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_hat).ravel()\n",
    "    return {\n",
    "        \"PR_AUC\": ap, \"ROC_AUC\": roc, \"precision@thr\": prec, \"recall@thr\": rec, \"F1@thr\": f1,\n",
    "        \"TP\": int(tp), \"FP\": int(fp), \"FN\": int(fn), \"TN\": int(tn), \"threshold\": float(threshold)\n",
    "    }\n",
    "\n",
    "def pick_threshold_for_min_precision(y_true, y_prob, min_precision=0.90):\n",
    "    prec, rec, thr = precision_recall_curve(y_true, y_prob)\n",
    "    mask = prec >= min_precision\n",
    "    if mask.any():\n",
    "        thr_ok = thr[mask].min()  # menor umbral que cumple la precisión mínima\n",
    "    else:\n",
    "        thr_ok = 0.5  # fallback\n",
    "    return float(thr_ok)\n",
    "\n",
    "def plot_pr_curves(models_probs, y_true):\n",
    "    plt.figure(figsize=(6,5))\n",
    "    for name, probs in models_probs.items():\n",
    "        prec, rec, _ = precision_recall_curve(y_true, probs)\n",
    "        ap = average_precision_score(y_true, probs)\n",
    "        plt.plot(rec, prec, label=f\"{name} (AP={ap:.3f})\")\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(\"Curva Precisión–Recall (OOF)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a03dab",
   "metadata": {},
   "source": [
    "## Definición de modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a0ea74",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.13.0' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: 'c:/Users/matir/AppData/Local/Programs/Python/Python313/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# ==== OPCIONAL: Cargar modelos ya entrenados desde archivos .pkl ====\n",
    "# Si querés ahorrar tiempo y ya tenés tus modelos entrenados, descomentá este bloque.\n",
    "\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "PRETRAINED_DIR = Path(\"/mnt/data\")  # o \"models\" si los moviste allí\n",
    "\n",
    "pretrained_files = {\n",
    "    \"RandomForest\": PRETRAINED_DIR / \"models/random_forest.pkl\",\n",
    "    \"ExtraTrees\": PRETRAINED_DIR / \"models/extra_trees.pkl\",\n",
    "    \"AdaBoost\": PRETRAINED_DIR / \"models/adaboost.pkl\",\n",
    "    \"Stacking\": PRETRAINED_DIR / \"models/stacking.pkl\",\n",
    "}\n",
    "\n",
    "loaded = {}\n",
    "for name, path in pretrained_files.items():\n",
    "    if path.exists():\n",
    "        try:\n",
    "            model = joblib.load(path)\n",
    "            loaded[name] = model\n",
    "            print(f\"✅ Cargado modelo preentrenado: {name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ No se pudo cargar {name}: {e}\")\n",
    "    else:\n",
    "        print(f\"⚠️ Archivo no encontrado para {name}: {path}\")\n",
    "\n",
    "if loaded:\n",
    "    models = loaded  # sobrescribe los modelos definidos más arriba\n",
    "    print(\"\\\\nUsando modelos preentrenados para evaluación.\")\n",
    "else:\n",
    "    print(\"\\\\nNo se encontraron modelos preentrenados, se usarán los definidos arriba.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb4ea6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== Modelos base ====\n",
    "rf  = RandomForestClassifier(n_estimators=600, criterion='entropy', random_state=42, n_jobs=-1, class_weight=None)\n",
    "et  = ExtraTreesClassifier(n_estimators=600, criterion='entropy', random_state=42, n_jobs=-1)\n",
    "ada = AdaBoostClassifier(n_estimators=600, learning_rate=0.1, random_state=42)\n",
    "\n",
    "# Meta-modelo para stacking\n",
    "meta = LogisticRegression(max_iter=2000, n_jobs=None)\n",
    "\n",
    "stack = StackingClassifier(\n",
    "    estimators=[('rf', rf), ('et', et), ('ada', ada)],\n",
    "    final_estimator=meta,\n",
    "    cv=cv,\n",
    "    passthrough=False,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "models = {\n",
    "    \"RandomForest\": rf,\n",
    "    \"ExtraTrees\": et,\n",
    "    \"AdaBoost\": ada,\n",
    "    \"Stacking\": stack\n",
    "}\n",
    "\n",
    "print(\"Modelos definidos:\", list(models.keys()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5069e250",
   "metadata": {},
   "source": [
    "## Evaluación OOF (mismo CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d4506b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== Evaluación OOF ====\n",
    "try:\n",
    "    X; y\n",
    "except NameError:\n",
    "    raise RuntimeError(\"Definí X,y antes de correr esta celda. Ver 'Carga de datos'.\")\n",
    "\n",
    "oof_probs = {}\n",
    "scores_tbl = []\n",
    "\n",
    "for name, clf in models.items():\n",
    "    # Si tu pipeline necesitara escalado, encadenalo aquí:\n",
    "    pipe = Pipeline([\n",
    "        # (\"scaler\", StandardScaler()),  # activar si hace falta\n",
    "        (\"clf\", clf)\n",
    "    ])\n",
    "    probs = cross_val_predict(pipe, X, y, cv=cv, method=\"predict_proba\")[:,1]\n",
    "    oof_probs[name] = probs\n",
    "    metrics = pr_metrics_from_probs(y, probs, threshold=0.5)\n",
    "    row = {\"model\": name, **metrics}\n",
    "    scores_tbl.append(row)\n",
    "    print(f\"{name}  -> PR-AUC={metrics['PR_AUC']:.4f} | ROC-AUC={metrics['ROC_AUC']:.4f} | \"\n",
    "          f\"Prec@0.5={metrics['precision@thr']:.3f} | Recall@0.5={metrics['recall@thr']:.3f}\")\n",
    "\n",
    "scores_df = pd.DataFrame(scores_tbl).sort_values(\"PR_AUC\", ascending=False).reset_index(drop=True)\n",
    "scores_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20170b4",
   "metadata": {},
   "source": [
    "### Curvas PR (comparación OOF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e846f43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_pr_curves(oof_probs, y_true=y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1e90ca",
   "metadata": {},
   "source": [
    "## Selección de modelo + umbral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221cd48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Elegimos el mejor por PR-AUC (primer fila del scores_df)\n",
    "best_name = scores_df.loc[0, \"model\"]\n",
    "print(\"Mejor por PR-AUC (OOF):\", best_name)\n",
    "\n",
    "# Umbral para cumplir precisión mínima (ajustá a tu objetivo)\n",
    "MIN_PRECISION = 0.90\n",
    "thr_best = pick_threshold_for_min_precision(y, oof_probs[best_name], min_precision=MIN_PRECISION)\n",
    "print(f\"Umbral elegido para {best_name} con precisión ≥ {MIN_PRECISION:.2f}: {thr_best:.3f}\")\n",
    "\n",
    "# Métricas en ese punto de operación\n",
    "op_metrics = pr_metrics_from_probs(y, oof_probs[best_name], threshold=thr_best)\n",
    "pd.DataFrame([op_metrics])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15de6c82",
   "metadata": {},
   "source": [
    "## Calibración y entrenamiento final (full data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af808ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calibramos el mejor modelo y lo entrenamos en el 100% de los datos\n",
    "base = models[best_name]\n",
    "pipe = Pipeline([\n",
    "    # (\"scaler\", StandardScaler()),  # activar si hace falta para tus features\n",
    "    (\"clf\", base)\n",
    "])\n",
    "\n",
    "calibrated = CalibratedClassifierCV(pipe, method=\"isotonic\", cv=cv)\n",
    "calibrated.fit(X, y)\n",
    "\n",
    "# Guardado\n",
    "final_model_path = MODELS_DIR / f\"{best_name.lower()}_calibrated.pkl\"\n",
    "joblib.dump(calibrated, final_model_path)\n",
    "print(\"✅ Modelo calibrado guardado en:\", final_model_path)\n",
    "\n",
    "# Guardamos parámetros operativos (umbral + columnas)\n",
    "export = {\n",
    "    \"model_name\": best_name,\n",
    "    \"threshold\": float(thr_best),\n",
    "    \"feature_cols\": feature_cols if feature_cols is not None else (list(range(X.shape[1])) if hasattr(X, 'shape') else None),\n",
    "    \"created_at\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "    \"min_precision_target\": float(MIN_PRECISION)\n",
    "}\n",
    "with open(MODELS_DIR / \"inference_config.json\", \"w\") as f:\n",
    "    json.dump(export, f, indent=2)\n",
    "print(\"✅ inference_config.json guardado.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db25b66",
   "metadata": {},
   "source": [
    "## Registry de experimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51058272",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Actualizamos/creamos registry.json con resultados OOF y punto de operación\n",
    "registry_entry = {\n",
    "    \"timestamp\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "    \"cv\": {\"n_splits\": 5, \"shuffle\": True, \"random_state\": 42},\n",
    "    \"scores\": scores_df.to_dict(orient=\"records\"),\n",
    "    \"selected_model\": best_name,\n",
    "    \"operating_point\": {\n",
    "        \"threshold\": float(thr_best),\n",
    "        **pr_metrics_from_probs(y, oof_probs[best_name], threshold=thr_best)\n",
    "    },\n",
    "    \"artifacts\": {\n",
    "        \"model_path\": str(final_model_path),\n",
    "        \"inference_config\": str(MODELS_DIR / \"inference_config.json\")\n",
    "    }\n",
    "}\n",
    "\n",
    "if REGISTRY_PATH.exists():\n",
    "    with open(REGISTRY_PATH, \"r\") as f:\n",
    "        try:\n",
    "            reg = json.load(f)\n",
    "            if not isinstance(reg, list): reg = [reg]\n",
    "        except Exception:\n",
    "            reg = []\n",
    "else:\n",
    "    reg = []\n",
    "\n",
    "reg.append(registry_entry)\n",
    "with open(REGISTRY_PATH, \"w\") as f:\n",
    "    json.dump(reg, f, indent=2)\n",
    "\n",
    "print(\"✅ Registry actualizado en\", REGISTRY_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5789c3",
   "metadata": {},
   "source": [
    "## Inferencia con umbral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7b5991",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== Inferencia ====\n",
    "def load_model_and_config(model_path=None, config_path=None):\n",
    "    model_path = model_path or (MODELS_DIR / f\"{best_name.lower()}_calibrated.pkl\")\n",
    "    config_path = config_path or (MODELS_DIR / \"inference_config.json\")\n",
    "    model = joblib.load(model_path)\n",
    "    with open(config_path, \"r\") as f:\n",
    "        cfg = json.load(f)\n",
    "    return model, cfg\n",
    "\n",
    "def predict_with_threshold(X_new, model, threshold):\n",
    "    probs = model.predict_proba(X_new)[:,1]\n",
    "    return (probs >= threshold).astype(int), probs\n",
    "\n",
    "# Ejemplo de uso (descomentar cuando tengas X_new):\n",
    "# model, cfg = load_model_and_config()\n",
    "# y_pred, y_prob = predict_with_threshold(X_new, model, cfg[\"threshold\"])\n",
    "# print(\"Preds:\", y_pred[:10], \"Probs:\", y_prob[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f817175",
   "metadata": {},
   "source": [
    "## Housekeeping (opcional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024c4c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Renombrado sugerido para mantener consistencia (ejecutar sólo si querés limpiar)\n",
    "# Ojo: no elimina nada, solo muestra propuestas.\n",
    "existing = list(MODELS_DIR.glob(\"*.pkl\"))\n",
    "print(\"Archivos actuales en models/:\")\n",
    "for p in existing:\n",
    "    print(\"-\", p.name)\n",
    "print(\"\\nSugerencia: conservar sólo los modelos calibrados y registrar métricas en registry.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
